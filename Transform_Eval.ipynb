{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8c63d6-0280-457e-ab28-1bdafda5e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LightningRepClassification, LightningTransformRegression, LightningRegression,create_model\n",
    "from model_info import encoders, modulators, model_output_dims\n",
    "from utils import set_seed, get_args\n",
    "import torch\n",
    "from datasets import IdSpritesEval\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(args):\n",
    "    indices = list(range(480000))\n",
    "    data = torch.load(f\"{args.dataset}/{args.dataset}.pth\", map_location=\"cpu\")\n",
    "    if args.pretrained_reps:\n",
    "        data['reps'] = torch.load(f\"{args.dataset}/{args.dataset}_images_feats_{args.pretrained_reps}.pth\", map_location=\"cpu\")\n",
    "    ds = IdSpritesEval(args, data, indices, max_delta=14, num_samples=20, p_skip=0, test=False, return_indices=True)\n",
    "    return ds\n",
    "    \n",
    "def get_dataloader(args, ds, indices):\n",
    "    sampler = SubsetRandomSampler(indices=indices)\n",
    "    dl = DataLoader(ds, batch_size=1024, sampler=sampler)\n",
    "    return dl\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    split = ['train','id','ood']\n",
    "\n",
    "    sss = []\n",
    "    tasks = []\n",
    "    _ , _ , src_rep, _, _, latents = next(iter(dataloader))\n",
    "    n_attrs = latents.shape[-1]\n",
    "    dims = src_rep.shape[-1]\n",
    "\n",
    "    device = \"cuda\"\n",
    "    y_squared = torch.zeros(n_attrs, dims).to(device)\n",
    "    ys = torch.zeros(n_attrs, dims).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        n_batches = 0\n",
    "        for n_batch, batch in enumerate(tqdm(dataloader)):\n",
    "            # Unpack index + batch\n",
    "            idxs, src_img, src_rep, imgs, gt_reps, latents = batch\n",
    "            idxs, src_img, src_rep, imgs, gt_reps, latents = idxs.cuda(),src_img.cuda(), src_rep.cuda(), imgs.cuda(), gt_reps.cuda(), latents.cuda()\n",
    "\n",
    "            n_batches+=1\n",
    "            data = model.split_step((src_img, src_rep, imgs, gt_reps, latents))\n",
    "            \n",
    "            sss.append(torch.sum((data['targets'] - data['logits']) ** 2, dim=1))\n",
    "            tasks.append(data['tasks'])\n",
    "\n",
    "            # rolling stats\n",
    "            for i in range(n_attrs):\n",
    "                y_squared[i] += (data['targets'][data['tasks'] == i]**2).sum(dim=0)\n",
    "                ys[i] += data['targets'][data['tasks'] == i].sum(dim=0)\n",
    "\n",
    "        ss_res = torch.cat(sss, dim=0).cuda()\n",
    "        tasks = torch.cat(tasks, dim=0).long().cuda()\n",
    "        \n",
    "        \n",
    "        n_attrs = 6\n",
    "        dtype = ss_res.dtype\n",
    "        device = ss_res.device\n",
    "        ss_res = torch.zeros(n_attrs, dtype=dtype, device=device).scatter_reduce(0,\n",
    "                                                                            tasks,\n",
    "                                                                            ss_res,\n",
    "                                                                            reduce=\"sum\")\n",
    "\n",
    "        counts = torch.zeros(n_attrs, dtype=tasks.dtype, device=device).scatter_reduce(0,tasks, torch.ones_like(tasks).cuda(), reduce=\"sum\").to(device)\n",
    "        mus = torch.empty(n_attrs, dims).to(device)\n",
    "        ss_tot = torch.empty(n_attrs).to(device)\n",
    "        for i in range(n_attrs):\n",
    "            mus[i] = ys[i]/counts[i]\n",
    "            ss_tot[i] = (y_squared[i] - 2*ys[i]*mus[i] + mus[i]**2).sum()  # ==> sum_{i=1 in T} (y_i - mu_T)^2\n",
    "\n",
    "        r2 = 1- ss_res/ss_tot                            # TODO: get this per task\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8311be4-52f1-45d6-9e65-030662521e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3a7d33402c4342bb2229ea94e4520e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001, 'wd': 0.04, 'arch': 'none', 'fovs': ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation'], 'seed': 111, 'test': False, 'frozen': True, 'losses': 'class', 'n_fovs': {'scale': 8, 'shape': 4, 'wall_hue': 10, 'floor_hue': 10, 'object_hue': 10, 'orientation': 15}, 'warmup': 6.666666666666667, 'dataset': '3dshapes', 'encoder': {'arch': 'none', 'frozen': True, 'enc_dims': 16, 'pretrained': None, 'pretrain_method': None}, 'data_dir': '/mnt/nas2/GrimaRepo/araymond', 'enc_dims': 16, 'final_lr': 1e-06, 'final_wd': 0.4, 'fovs_ids': [0, 1, 2, 3, 4, 5], 'mod_arch': 'mlp', 'mod_dims': 16, 'start_lr': 0.0002, 'train_bs': 256, 'ema_start': 0.996, 'ipe_scale': 1, 'modulator': {'arch': 'mlp', 'hidden_dim': 16}, 'num_steps': 400000, 'resume_id': None, 'fovs_tasks': ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation'], 'num_epochs': 50, 'save_every': 10, 'fovs_levels': {'3dshapes': {'scale': 3, 'shape': 2, 'wall_hue': 2, 'floor_hue': 2, 'object_hue': 2, 'orientation': 3}, 'idsprites': {'x': 3, 'y': 3, 'scale': 3, 'shape': 3, 'orientation': 3}}, 'num_workers': 4, 'sub_dataset': 'extrapolation', 'fovs_indices': {'scale': 3, 'shape': 4, 'wall_hue': 1, 'floor_hue': 0, 'object_hue': 2, 'orientation': 5}, 'save_metrics': True, 'save_weights': True, 'train_method': 'transform', 'experiment_id': None, 'iters_per_ema': 1, 'pretrain_method': None, 'pretrained_arch': None, 'pretrained_reps': 'vit_l_32', 'FOVS_PER_DATASET': ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation'], 'pretrained_feats': False, 'pretrained_encoder': None, 'task_to_label_index': {'scale': 3, 'shape': 4, 'wall_hue': 1, 'floor_hue': 0, 'object_hue': 2, 'orientation': 5}}\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cff0d6fcae45d1b3a843bcc2ea704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886a13f794cf44b7adcfaa5701c24dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ebd3ffb2854e78a5d441337c8ce0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f039fad02164bc69c88a73a6fb5292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001, 'wd': 0.04, 'arch': 'none', 'fovs': ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation'], 'seed': 111, 'test': False, 'frozen': True, 'losses': 'class', 'n_fovs': {'scale': 8, 'shape': 4, 'wall_hue': 10, 'floor_hue': 10, 'object_hue': 10, 'orientation': 15}, 'warmup': 6.666666666666667, 'dataset': '3dshapes', 'encoder': {'arch': 'none', 'frozen': True, 'enc_dims': 16, 'pretrained': None, 'pretrain_method': None}, 'data_dir': '/mnt/nas2/GrimaRepo/araymond', 'enc_dims': 16, 'final_lr': 1e-06, 'final_wd': 0.4, 'fovs_ids': [0, 1, 2, 3, 4, 5], 'mod_arch': 'mlp', 'mod_dims': 16, 'start_lr': 0.0002, 'train_bs': 256, 'ema_start': 0.996, 'ipe_scale': 1, 'modulator': {'arch': 'mlp', 'hidden_dim': 16}, 'num_steps': 400000, 'resume_id': None, 'fovs_tasks': ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation'], 'num_epochs': 50, 'save_every': 10, 'fovs_levels': {'3dshapes': {'scale': 3, 'shape': 2, 'wall_hue': 2, 'floor_hue': 2, 'object_hue': 2, 'orientation': 3}, 'idsprites': {'x': 3, 'y': 3, 'scale': 3, 'shape': 3, 'orientation': 3}}, 'num_workers': 4, 'sub_dataset': 'extrapolation', 'fovs_indices': {'scale': 3, 'shape': 4, 'wall_hue': 1, 'floor_hue': 0, 'object_hue': 2, 'orientation': 5}, 'save_metrics': True, 'save_weights': True, 'train_method': 'transform', 'experiment_id': None, 'iters_per_ema': 1, 'pretrain_method': None, 'pretrained_arch': None, 'pretrained_reps': 'vit_b_32', 'FOVS_PER_DATASET': ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation'], 'pretrained_feats': False, 'pretrained_encoder': None, 'task_to_label_index': {'scale': 3, 'shape': 4, 'wall_hue': 1, 'floor_hue': 0, 'object_hue': 2, 'orientation': 5}}\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56583ae66bab484181743014d0ab9211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dfdc4c098f463ca8c9565b418646fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286911f3b6c942a79b2cac37b0a0629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05755b4635c74e81919727c56aefd6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define experiment ids for checkpoint\n",
    "\n",
    "exps = [#'0hdoi4lw', # composition\n",
    "       #'6dlybv9s', # composition\n",
    "       # \"dbnxlnfv\", # interpolation\n",
    "       # \"tt466w4y\", # interpolation\n",
    "        \"te4t8cr4\", # extrapolation\n",
    "        \"n0vdpha1\" # extrapolation\n",
    "       ]\n",
    "\n",
    "for exp_id in tqdm(exps):\n",
    "    args = get_args(exp_id)\n",
    "    args.encoder['pretrain_method'] = None\n",
    "    print(args)\n",
    "\n",
    "    ds = get_dataset(args)\n",
    "    df = pd.DataFrame()\n",
    "    for split in tqdm(['train','id','ood']):\n",
    "        encoder, modulator = create_model(args)\n",
    "        model = LightningTransformRegression.load_from_checkpoint(checkpoint_path=f\"results/{args.dataset}/{exp_id}/last.ckpt\", \n",
    "                                            args=args, \n",
    "                                            encoder=encoder, \n",
    "                                            modulator=modulator)\n",
    "        if split in ['train','id']:\n",
    "            indices = torch.load(f\"3dshapes/shapes3d_{args.sub_dataset}_train_indices.pth\")\n",
    "            train_indices, val_indices = train_test_split(indices, test_size = 0.1, random_state=42)\n",
    "            indices = train_indices if split == \"train\" else val_indices\n",
    "        elif split == \"ood\":\n",
    "            indices = torch.load(f\"3dshapes/shapes3d_{args.sub_dataset}_test_indices.pth\")\n",
    "        else:\n",
    "            print(\"Split not recognized!\")\n",
    "            indices = torch.tensor([]).long()\n",
    "            \n",
    "        dl = get_dataloader(args, ds, indices)\n",
    "        r2 = evaluate(model, dl)\n",
    "        \n",
    "        # Store metadata\n",
    "        model_name = args.pretrained_reps\n",
    "        if args.pretrained_reps is None:\n",
    "            if args.pretrained_encoder is not None:\n",
    "                enc_args = get_args(args.pretrained_encoder)\n",
    "                model_name = enc_args.pretrained_reps \n",
    "        meta = {\n",
    "            'split': split,\n",
    "            'dataset': args.dataset,\n",
    "            'sub_dataset': args.sub_dataset,\n",
    "            'model': f\"{model_name}\"\n",
    "        }\n",
    "        \n",
    "        # Create a long-format DataFrame where each row is a (task, r2) pair\n",
    "        rows = []\n",
    "        for i, fov in enumerate(args.FOVS_PER_DATASET):\n",
    "            rows.append({\n",
    "                **meta,\n",
    "                'task': fov,\n",
    "                'r2': r2[i].item()\n",
    "            })\n",
    "\n",
    "        # Append to df\n",
    "        result_df = pd.DataFrame(rows)\n",
    "        df = pd.concat([df, result_df], ignore_index=True)\n",
    "\n",
    "    df.to_csv(f\"transform_{exp_id}_{args.dataset}_{args.sub_dataset}_{args.pretrained_reps}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6723301d-fc48-4e2e-8a8e-58a872d68a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ['0hdoi4lw', # composition\n",
    "       '6dlybv9s', # composition\n",
    "        \"dbnxlnfv\", # interpolation\n",
    "        \"tt466w4y\", # interpolation\n",
    "        \"te4t8cr4\", # extrapolation\n",
    "        \"n0vdpha1\" # extrapolation\n",
    "       ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b19420-50c7-4024-bf3f-8dc9d511c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset  sub_dataset     model  split         task        r2\n",
      "0   3dshapes  composition  vit_l_32     id    floor_hue  0.820189\n",
      "1   3dshapes  composition  vit_l_32     id   object_hue  0.817173\n",
      "2   3dshapes  composition  vit_l_32     id  orientation  0.844980\n",
      "3   3dshapes  composition  vit_l_32     id        scale  0.856303\n",
      "4   3dshapes  composition  vit_l_32     id        shape  0.837392\n",
      "5   3dshapes  composition  vit_l_32     id     wall_hue  0.810326\n",
      "6   3dshapes  composition  vit_l_32    ood    floor_hue  0.755349\n",
      "7   3dshapes  composition  vit_l_32    ood   object_hue  0.748533\n",
      "8   3dshapes  composition  vit_l_32    ood  orientation  0.782774\n",
      "9   3dshapes  composition  vit_l_32    ood        scale  0.789272\n",
      "10  3dshapes  composition  vit_l_32    ood        shape  0.755803\n",
      "11  3dshapes  composition  vit_l_32    ood     wall_hue  0.746183\n",
      "12  3dshapes  composition  vit_l_32  train    floor_hue  0.821755\n",
      "13  3dshapes  composition  vit_l_32  train   object_hue  0.819229\n",
      "14  3dshapes  composition  vit_l_32  train  orientation  0.846408\n",
      "15  3dshapes  composition  vit_l_32  train        scale  0.857921\n",
      "16  3dshapes  composition  vit_l_32  train        shape  0.839274\n",
      "17  3dshapes  composition  vit_l_32  train     wall_hue  0.811786\n",
      "     dataset  sub_dataset     model  split         task        r2\n",
      "0   3dshapes  composition  vit_b_32     id    floor_hue  0.830173\n",
      "1   3dshapes  composition  vit_b_32     id   object_hue  0.833310\n",
      "2   3dshapes  composition  vit_b_32     id  orientation  0.866473\n",
      "3   3dshapes  composition  vit_b_32     id        scale  0.874353\n",
      "4   3dshapes  composition  vit_b_32     id        shape  0.851716\n",
      "5   3dshapes  composition  vit_b_32     id     wall_hue  0.827378\n",
      "6   3dshapes  composition  vit_b_32    ood    floor_hue  0.762862\n",
      "7   3dshapes  composition  vit_b_32    ood   object_hue  0.758141\n",
      "8   3dshapes  composition  vit_b_32    ood  orientation  0.800964\n",
      "9   3dshapes  composition  vit_b_32    ood        scale  0.808458\n",
      "10  3dshapes  composition  vit_b_32    ood        shape  0.771676\n",
      "11  3dshapes  composition  vit_b_32    ood     wall_hue  0.760987\n",
      "12  3dshapes  composition  vit_b_32  train    floor_hue  0.831100\n",
      "13  3dshapes  composition  vit_b_32  train   object_hue  0.834841\n",
      "14  3dshapes  composition  vit_b_32  train  orientation  0.867303\n",
      "15  3dshapes  composition  vit_b_32  train        scale  0.875319\n",
      "16  3dshapes  composition  vit_b_32  train        shape  0.852657\n",
      "17  3dshapes  composition  vit_b_32  train     wall_hue  0.828164\n",
      "     dataset    sub_dataset     model  split         task        r2\n",
      "0   3dshapes  interpolation  vit_b_32     id    floor_hue  0.836922\n",
      "1   3dshapes  interpolation  vit_b_32     id   object_hue  0.834801\n",
      "2   3dshapes  interpolation  vit_b_32     id  orientation  0.874824\n",
      "3   3dshapes  interpolation  vit_b_32     id        scale  0.872758\n",
      "4   3dshapes  interpolation  vit_b_32     id        shape  0.864299\n",
      "5   3dshapes  interpolation  vit_b_32     id     wall_hue  0.829349\n",
      "6   3dshapes  interpolation  vit_b_32    ood    floor_hue  0.723726\n",
      "7   3dshapes  interpolation  vit_b_32    ood   object_hue  0.725931\n",
      "8   3dshapes  interpolation  vit_b_32    ood  orientation  0.809086\n",
      "9   3dshapes  interpolation  vit_b_32    ood        scale  0.815702\n",
      "10  3dshapes  interpolation  vit_b_32    ood        shape  0.785811\n",
      "11  3dshapes  interpolation  vit_b_32    ood     wall_hue  0.706612\n",
      "12  3dshapes  interpolation  vit_b_32  train    floor_hue  0.838207\n",
      "13  3dshapes  interpolation  vit_b_32  train   object_hue  0.836013\n",
      "14  3dshapes  interpolation  vit_b_32  train  orientation  0.875650\n",
      "15  3dshapes  interpolation  vit_b_32  train        scale  0.873871\n",
      "16  3dshapes  interpolation  vit_b_32  train        shape  0.865546\n",
      "17  3dshapes  interpolation  vit_b_32  train     wall_hue  0.830449\n",
      "     dataset    sub_dataset     model  split         task        r2\n",
      "0   3dshapes  interpolation  vit_l_32     id    floor_hue  0.819529\n",
      "1   3dshapes  interpolation  vit_l_32     id   object_hue  0.813959\n",
      "2   3dshapes  interpolation  vit_l_32     id  orientation  0.849522\n",
      "3   3dshapes  interpolation  vit_l_32     id        scale  0.847024\n",
      "4   3dshapes  interpolation  vit_l_32     id        shape  0.842896\n",
      "5   3dshapes  interpolation  vit_l_32     id     wall_hue  0.802319\n",
      "6   3dshapes  interpolation  vit_l_32    ood    floor_hue  0.710778\n",
      "7   3dshapes  interpolation  vit_l_32    ood   object_hue  0.702448\n",
      "8   3dshapes  interpolation  vit_l_32    ood  orientation  0.779746\n",
      "9   3dshapes  interpolation  vit_l_32    ood        scale  0.791631\n",
      "10  3dshapes  interpolation  vit_l_32    ood        shape  0.766037\n",
      "11  3dshapes  interpolation  vit_l_32    ood     wall_hue  0.681574\n",
      "12  3dshapes  interpolation  vit_l_32  train    floor_hue  0.820849\n",
      "13  3dshapes  interpolation  vit_l_32  train   object_hue  0.815720\n",
      "14  3dshapes  interpolation  vit_l_32  train  orientation  0.850874\n",
      "15  3dshapes  interpolation  vit_l_32  train        scale  0.847670\n",
      "16  3dshapes  interpolation  vit_l_32  train        shape  0.845315\n",
      "17  3dshapes  interpolation  vit_l_32  train     wall_hue  0.804275\n",
      "     dataset    sub_dataset     model  split         task        r2\n",
      "0   3dshapes  extrapolation  vit_l_32     id    floor_hue  0.802029\n",
      "1   3dshapes  extrapolation  vit_l_32     id   object_hue  0.798602\n",
      "2   3dshapes  extrapolation  vit_l_32     id  orientation  0.821899\n",
      "3   3dshapes  extrapolation  vit_l_32     id        scale  0.832293\n",
      "4   3dshapes  extrapolation  vit_l_32     id        shape  0.819893\n",
      "5   3dshapes  extrapolation  vit_l_32     id     wall_hue  0.795141\n",
      "6   3dshapes  extrapolation  vit_l_32    ood    floor_hue  0.648711\n",
      "7   3dshapes  extrapolation  vit_l_32    ood   object_hue  0.644427\n",
      "8   3dshapes  extrapolation  vit_l_32    ood  orientation  0.688217\n",
      "9   3dshapes  extrapolation  vit_l_32    ood        scale  0.719031\n",
      "10  3dshapes  extrapolation  vit_l_32    ood        shape  0.679680\n",
      "11  3dshapes  extrapolation  vit_l_32    ood     wall_hue  0.634308\n",
      "12  3dshapes  extrapolation  vit_l_32  train    floor_hue  0.803610\n",
      "13  3dshapes  extrapolation  vit_l_32  train   object_hue  0.799941\n",
      "14  3dshapes  extrapolation  vit_l_32  train  orientation  0.822774\n",
      "15  3dshapes  extrapolation  vit_l_32  train        scale  0.833655\n",
      "16  3dshapes  extrapolation  vit_l_32  train        shape  0.821170\n",
      "17  3dshapes  extrapolation  vit_l_32  train     wall_hue  0.797375\n",
      "     dataset    sub_dataset     model  split         task        r2\n",
      "0   3dshapes  extrapolation  vit_b_32     id    floor_hue  0.823776\n",
      "1   3dshapes  extrapolation  vit_b_32     id   object_hue  0.830696\n",
      "2   3dshapes  extrapolation  vit_b_32     id  orientation  0.846650\n",
      "3   3dshapes  extrapolation  vit_b_32     id        scale  0.859351\n",
      "4   3dshapes  extrapolation  vit_b_32     id        shape  0.849552\n",
      "5   3dshapes  extrapolation  vit_b_32     id     wall_hue  0.822614\n",
      "6   3dshapes  extrapolation  vit_b_32    ood    floor_hue  0.666047\n",
      "7   3dshapes  extrapolation  vit_b_32    ood   object_hue  0.669411\n",
      "8   3dshapes  extrapolation  vit_b_32    ood  orientation  0.719593\n",
      "9   3dshapes  extrapolation  vit_b_32    ood        scale  0.739943\n",
      "10  3dshapes  extrapolation  vit_b_32    ood        shape  0.701638\n",
      "11  3dshapes  extrapolation  vit_b_32    ood     wall_hue  0.671840\n",
      "12  3dshapes  extrapolation  vit_b_32  train    floor_hue  0.824809\n",
      "13  3dshapes  extrapolation  vit_b_32  train   object_hue  0.831165\n",
      "14  3dshapes  extrapolation  vit_b_32  train  orientation  0.846902\n",
      "15  3dshapes  extrapolation  vit_b_32  train        scale  0.860495\n",
      "16  3dshapes  extrapolation  vit_b_32  train        shape  0.850594\n",
      "17  3dshapes  extrapolation  vit_b_32  train     wall_hue  0.824145\n"
     ]
    }
   ],
   "source": [
    "final_result = pd.DataFrame()\n",
    "for exp_id in exps:\n",
    "    args = get_args(exp_id)\n",
    "    filename = f\"transform_{exp_id}_{args.dataset}_{args.sub_dataset}_{args.pretrained_reps}.csv\"\n",
    "    exp_id, dataset, sub_dataset = filename.split(\"_\")[:3]\n",
    "    pretrained_reps = \"-\".join(filename.split(\"_\")[3:]).replace(\".csv\",\"\")\n",
    "    df= pd.read_csv(filename)\n",
    "    result = df.groupby(['dataset','sub_dataset',\"model\", 'split', 'task'])['r2'].mean().reset_index()\n",
    "    result.rename(columns={'correct': 'accuracy'}, inplace=True)\n",
    "    print(result)\n",
    "    final_result = pd.concat([final_result, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5403f8-911f-4c2d-828d-94ee1ec78c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['split'] = pd.Categorical(final_result['split'], categories=['train', 'id','ood'], ordered=True)\n",
    "final_result = final_result.sort_values(by=['dataset', 'sub_dataset','model','split'], ascending=[True, True,True,True])\n",
    "\n",
    "models = [\"train\",\"id\",\"ood\"]\n",
    "df = final_result\n",
    "tables = {}\n",
    "for model in models:\n",
    "    df_model = df[df['split'] == model]\n",
    "    pivot = df_model.pivot_table(\n",
    "        index=['sub_dataset', 'model'],\n",
    "        columns='task',\n",
    "        values='r2'\n",
    "    ).reset_index()\n",
    "    tables[model] = pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6c762b7-ff3f-49ba-86c4-344fdc56657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[ht]\n",
      "\\centering\n",
      "\\caption{Hola}\n",
      "\\label{tab:asda}\n",
      "\\begin{tabularx}{\\textwidth}{llXXXXXX}\n",
      "\\textbf{Sub dataset} & \\textbf{Model} & \\textbf{Floor hue} & \\textbf{Object hue} & \\textbf{Orientation} & \\textbf{Scale} & \\textbf{Shape} & \\textbf{Wall hue} \\\\\n",
      "Composition & VIT_B_32 & 0.83 & 0.83 & 0.87 & 0.87 & 0.85 & 0.83 \\\\\n",
      "Composition & VIT_L_32 & 0.82 & 0.82 & 0.84 & 0.86 & 0.84 & 0.81 \\\\\n",
      "Extrapolation & VIT_B_32 & 0.82 & 0.83 & 0.85 & 0.86 & 0.85 & 0.82 \\\\\n",
      "Extrapolation & VIT_L_32 & 0.80 & 0.80 & 0.82 & 0.83 & 0.82 & 0.80 \\\\\n",
      "Interpolation & VIT_B_32 & 0.84 & 0.83 & 0.87 & 0.87 & 0.86 & 0.83 \\\\\n",
      "Interpolation & VIT_L_32 & 0.82 & 0.81 & 0.85 & 0.85 & 0.84 & 0.80 \\\\\n",
      "\\end{tabularx}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def df_to_tabularx(df, label, caption, column_width='\\\\textwidth'):\n",
    "    import io\n",
    "\n",
    "    # Copy and format DataFrame\n",
    "    df_fmt = df.copy()\n",
    "    df_fmt.iloc[:, 0] = df_fmt.iloc[:, 0].str.capitalize()  # sub_dataset\n",
    "    df_fmt.iloc[:, 1] = df_fmt.iloc[:, 1].str.upper()       # group\n",
    "\n",
    "    # Format numeric columns as percentages\n",
    "    for col in df.columns[2:]:\n",
    "        df_fmt[col] = (df[col]).map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "    # Build tabularx column format\n",
    "    num_task_columns = df_fmt.shape[1] - 2\n",
    "    column_format = 'll' + 'X' * num_task_columns\n",
    "\n",
    "    # Prepare bold column headers\n",
    "    bold_headers = [f\"\\\\textbf{{{col.replace('_', ' ').capitalize()}}}\" for col in df_fmt.columns]\n",
    "\n",
    "    # Use to_latex to get the content, skipping header\n",
    "    buf = io.StringIO()\n",
    "    df_fmt.to_latex(\n",
    "        buf,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        escape=False,\n",
    "        column_format=column_format\n",
    "    )\n",
    "    lines = buf.getvalue().splitlines()\n",
    "\n",
    "    # Insert bold header manually\n",
    "    header_line = ' & '.join(bold_headers) + ' \\\\\\\\'\n",
    "    table_body = '\\n'.join(lines[3:-2])  # skip to_latex's \\toprule, etc.\n",
    "\n",
    "    # Final LaTeX table\n",
    "    latex = (\n",
    "        f\"\\\\begin{{table}}[ht]\\n\"\n",
    "        f\"\\\\centering\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\"\n",
    "        f\"\\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\begin{{tabularx}}{{{column_width}}}{{{column_format}}}\\n\"\n",
    "        f\"{header_line}\\n\"\n",
    "        f\"{table_body}\\n\"\n",
    "        f\"\\\\end{{tabularx}}\\n\"\n",
    "        f\"\\\\end{{table}}\"\n",
    "    )\n",
    "\n",
    "    return latex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label = \"tab:asda\"\n",
    "caption = \"Hola\"\n",
    "print(df_to_tabularx(tables['id'], label, caption, column_width='\\\\textwidth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f33a0bcb-fd2e-4393-80c1-46e5f2c01bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([59549.7578, 60457.2773, 74756.4141, 50006.6914, 74390.8359, 71710.8906])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6376f13-3c64-4ce6-8b80-36ab6cbe6386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>sub_dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>floor_hue</th>\n",
       "      <th>object_hue</th>\n",
       "      <th>orientation</th>\n",
       "      <th>scale</th>\n",
       "      <th>shape</th>\n",
       "      <th>wall_hue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>composition</td>\n",
       "      <td>vit_b_32</td>\n",
       "      <td>0.830173</td>\n",
       "      <td>0.833310</td>\n",
       "      <td>0.866473</td>\n",
       "      <td>0.874353</td>\n",
       "      <td>0.851716</td>\n",
       "      <td>0.827378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>composition</td>\n",
       "      <td>vit_l_32</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>0.817173</td>\n",
       "      <td>0.844980</td>\n",
       "      <td>0.856303</td>\n",
       "      <td>0.837392</td>\n",
       "      <td>0.810326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>extrapolation</td>\n",
       "      <td>vit_b_32</td>\n",
       "      <td>0.823776</td>\n",
       "      <td>0.830696</td>\n",
       "      <td>0.846650</td>\n",
       "      <td>0.859351</td>\n",
       "      <td>0.849552</td>\n",
       "      <td>0.822614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extrapolation</td>\n",
       "      <td>vit_l_32</td>\n",
       "      <td>0.802029</td>\n",
       "      <td>0.798602</td>\n",
       "      <td>0.821899</td>\n",
       "      <td>0.832293</td>\n",
       "      <td>0.819893</td>\n",
       "      <td>0.795141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interpolation</td>\n",
       "      <td>vit_b_32</td>\n",
       "      <td>0.836922</td>\n",
       "      <td>0.834801</td>\n",
       "      <td>0.874824</td>\n",
       "      <td>0.872758</td>\n",
       "      <td>0.864299</td>\n",
       "      <td>0.829349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interpolation</td>\n",
       "      <td>vit_l_32</td>\n",
       "      <td>0.819529</td>\n",
       "      <td>0.813959</td>\n",
       "      <td>0.849522</td>\n",
       "      <td>0.847024</td>\n",
       "      <td>0.842896</td>\n",
       "      <td>0.802319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task    sub_dataset     model  floor_hue  object_hue  orientation     scale  \\\n",
       "0       composition  vit_b_32   0.830173    0.833310     0.866473  0.874353   \n",
       "1       composition  vit_l_32   0.820189    0.817173     0.844980  0.856303   \n",
       "2     extrapolation  vit_b_32   0.823776    0.830696     0.846650  0.859351   \n",
       "3     extrapolation  vit_l_32   0.802029    0.798602     0.821899  0.832293   \n",
       "4     interpolation  vit_b_32   0.836922    0.834801     0.874824  0.872758   \n",
       "5     interpolation  vit_l_32   0.819529    0.813959     0.849522  0.847024   \n",
       "\n",
       "task     shape  wall_hue  \n",
       "0     0.851716  0.827378  \n",
       "1     0.837392  0.810326  \n",
       "2     0.849552  0.822614  \n",
       "3     0.819893  0.795141  \n",
       "4     0.864299  0.829349  \n",
       "5     0.842896  0.802319  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b969e5-5bef-439b-bb80-d0d776bf014c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
