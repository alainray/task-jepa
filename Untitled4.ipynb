{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6719da7-f520-4d0a-8f5e-6117893f93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model_info import BigMultiHeadClassifier, BigDecoder\n",
    "from utils import get_args, get_name_from_args\n",
    "from models import get_model_from_exp\n",
    "import argparse\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_by_delta(model, classifier, dataloader, ood, device=\"cuda\", idx=0):\n",
    "    def latent_to_index(latents):\n",
    "        return torch.matmul(latents, latents_bases.T).sum(dim=-1).long()\n",
    "\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    acc_by_delta_and_gen = defaultdict(lambda: {\n",
    "        \"correct\": [[0]*6 for _ in range(4)],  # 4 categories x 6 heads\n",
    "        \"total\": [[0]*6 for _ in range(4)]\n",
    "    })\n",
    "\n",
    "    for n_batch, (reps, target_latents, deltas) in enumerate(tqdm(dataloader)):\n",
    "        reps = reps.to(device)\n",
    "        deltas = deltas.to(device)\n",
    "        target_latents = target_latents.to(device)\n",
    "\n",
    "        pred_reps = model.modulator(reps, deltas)\n",
    "        logits = classifier(pred_reps)  # list of 6 (B, num_classes)\n",
    "        preds = [torch.argmax(logit, dim=1) for logit in logits]\n",
    "        targets = [target_latents[:, i] for i in range(6)]\n",
    "        delta_mags = deltas[:, idx].abs().tolist()\n",
    "\n",
    "        for i in range(len(reps)):\n",
    "            delta_mag = int(delta_mags[i])\n",
    "            source_idx = latent_to_index(target_latents[i] - deltas[i])\n",
    "            target_idx = latent_to_index(target_latents[i])\n",
    "\n",
    "            source_ood = ood[source_idx].item()\n",
    "            target_ood = ood[target_idx].item()\n",
    "            category = int(source_ood * 2 + target_ood)\n",
    "\n",
    "            for j in range(6):\n",
    "                if preds[j][i].item() == targets[j][i].item():\n",
    "                    acc_by_delta_and_gen[delta_mag][\"correct\"][category][j] += 1\n",
    "                acc_by_delta_and_gen[delta_mag][\"total\"][category][j] += 1\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    cat_map = {0: \"iid-iid\", 1: \"iid-ood\", 2: \"ood-iid\", 3: \"ood-ood\"}\n",
    "\n",
    "    for delta, stats in acc_by_delta_and_gen.items():\n",
    "        for category in range(4):\n",
    "            for head in range(6):\n",
    "                row = {\n",
    "                    \"delta\": delta,\n",
    "                    \"category\": cat_map[category],\n",
    "                    \"head\": head_names[head],\n",
    "                    \"correct\": stats[\"correct\"][category][head],\n",
    "                    \"total\": stats[\"total\"][category][head],\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def generate_valid_deltas(base_latent, idx, max_delta=15):\n",
    "    base_val = int(base_latent[idx])  # ensure it's an integer\n",
    "    deltas = [torch.zeros(6)]\n",
    "\n",
    "    for d in range(1, max_delta + 1):\n",
    "        for sign in [-1, 1]:\n",
    "            new_val = base_val + sign * d\n",
    "            if 0 <= new_val < latent_dims[idx]:\n",
    "                delta = torch.zeros(6)\n",
    "                delta[idx] = sign * d\n",
    "                deltas.append(delta)\n",
    "\n",
    "    return torch.stack(deltas) if deltas else torch.zeros(0, 6)\n",
    "\n",
    "def collate_fn_with_targets(batch, idx=0, max_delta=3):\n",
    "    images_out = []\n",
    "    targets_out = []\n",
    "    deltas_out = []\n",
    "\n",
    "    for image, base_latent in batch:\n",
    "        base_latent = base_latent.float()\n",
    "        deltas = generate_valid_deltas(base_latent, idx, max_delta)\n",
    "\n",
    "        if deltas.shape[0] == 0:\n",
    "            continue  # skip if no valid deltas\n",
    "\n",
    "        for delta in deltas:\n",
    "            target_latent = base_latent + delta\n",
    "            images_out.append(image)\n",
    "            targets_out.append(target_latent)\n",
    "            deltas_out.append(delta)\n",
    "\n",
    "    return (\n",
    "        torch.stack(images_out),\n",
    "        torch.stack(targets_out),\n",
    "        torch.stack(deltas_out),\n",
    "    )\n",
    "\n",
    "# START OF MAIN LOOP\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Example of argparse usage\")\n",
    "\n",
    "parser.add_argument('--exp_id', type=str, help='Architecture to extract features from')\n",
    "input_args = parser.parse_args()\n",
    "\n",
    "latent_dims = [10, 10, 10, 8, 4, 15] # TODO: Change to adapt to dataset, this is 3dshapes\n",
    "max_delta = 15                       # how far to go from base_latent[i] \n",
    "# Get necessary models:\n",
    "# model: trained model that we want to evaluate, depends on exp_id\n",
    "# classifier: pretrained_model that is able to classsify correctly the latent attributes\n",
    "# decoder: specifiic model that decodes representations from model to create images to be evaluated\n",
    "# by classifier.\n",
    "exp_id = input_args.exp_id\n",
    "args = get_args(exp_id, update_id=True)\n",
    "\n",
    "latents_sizes = torch.tensor([10,10,10,8,4,15])\n",
    "latents_bases = torch.cat((latents_sizes.flip(0).cumprod(0).flip(0)[1:], torch.tensor([1])))\n",
    "base_latent = torch.tensor([9, 9, 9, 7, 3, 14])\n",
    "# Let's load decoder and classifier for evaluation\n",
    "device=\"cuda\"\n",
    "classifier = BigMultiHeadClassifier(d_hidden=128, use_encoder=False,num_blocks=4)\n",
    "\n",
    "filename = f\"{exp_id}_classifier_reps.pth\"\n",
    "\n",
    "weights = torch.load(f\"results/classifiers/{args.dataset}/{filename}\")\n",
    "classifier.load_state_dict(weights)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# Decoder\n",
    "args = get_args(exp_id, update_id=True)\n",
    "weights=torch.load(f\"results/decoders/{args.dataset}/{exp_id}_decoder.pth\")\n",
    "#exp_id = \"e99wn9i9\"\n",
    "#weights=torch.load(f\"results/decoders/{args.dataset}/full_decoder.pth\")\n",
    "device = \"cuda\"\n",
    "# Model to evaluate\n",
    "model = get_model_from_exp(args).to(device)\n",
    "# Get pretrained representations from ENCODER\n",
    "if args.pretrained_reps:\n",
    "    reps_path = args.pretrained_reps\n",
    "elif args.pretrained_encoder:\n",
    "    encoder_args = get_args(args.pretrained_encoder)\n",
    "    reps_path = encoder_args.pretrained_reps\n",
    "reps= torch.load(f\"{args.dataset}/{args.dataset}_images_feats_{reps_path}.pth\", map_location=\"cpu\") if reps_path else None\n",
    "reps = reps - reps.mean(dim=0) # center\n",
    "reps = torch.nn.functional.normalize(reps, p=2.0, dim=1, eps=1e-12)\n",
    "latents =  torch.load(f\"{args.dataset}/{args.dataset}.pth\")['latent_ids']\n",
    "ds = TensorDataset(reps, latents)\n",
    "\n",
    "# Let's load indices of what's ID and OOD\n",
    "# Obtener qué imagenes son ID y cuáles OOD\n",
    "test_indices = torch.load(f\"{args.dataset}/{args.dataset}_{args.sub_dataset}_test_indices.pth\")\n",
    "indices = torch.zeros(len(ds))\n",
    "indices[test_indices] = 1\n",
    "\n",
    "# Create DataLoaders\n",
    "FOVS_PER_DATASET = {'3dshapes': ['floor_hue','wall_hue','object_hue','scale','shape', 'orientation'],\n",
    "                'idsprites': [\"shape\",\"scale\",\"orientation\",\"x\",\"y\"],\n",
    "                \"dsprites\":  [\"shape\",\"scale\",\"orientation\",\"x\",\"y\"],\n",
    "                \"mpi3d\": ['object_color','object_shape','object_size','camera_height','background_color','h_axis','v_axis']\n",
    "                }\n",
    "head_names = FOVS_PER_DATASET[args.dataset]\n",
    "\n",
    "results_acc = pd.DataFrame()\n",
    "latents_bases = torch.cat((latents_sizes.flip(0).cumprod(0).flip(0)[1:], torch.tensor([1]))).float().to(\"cuda\")\n",
    "\n",
    "for idx in range(len(latent_dims)):\n",
    "    dl = DataLoader(ds, batch_size=128, shuffle=False, collate_fn=partial(collate_fn_with_targets, idx=idx, max_delta=15))\n",
    "\n",
    "    print(f\"Evaluating for latent {head_names[idx]}\",flush=True)\n",
    "    #acc_summary, overall_acc = eval_by_delta(model, classifier, decoder, dl, device=\"cuda\", idx=idx) \n",
    "    df_delta = eval_by_delta(model, classifier, dl, indices, device=\"cuda\", idx=idx)\n",
    "    modulated_factor = head_names[idx]  # idx = modulated dim passed to collate_fn\n",
    "\n",
    "    # Per-delta accuracy\n",
    "\n",
    "    df_delta[\"modulated_factor\"] = modulated_factor\n",
    "    df_delta['dataset'] = args.dataset\n",
    "    df_delta['sub_dataset'] = args.sub_dataset\n",
    "    df_delta['method'] = get_name_from_args(args)\n",
    "    df_delta['seed'] = args.seed\n",
    "\n",
    "    results_acc = pd.concat([results_acc, df_delta], ignore_index=True)\n",
    "    print(results_acc, flush=True)\n",
    "\n",
    "    results_acc.to_csv(f\"results/{args.dataset}/{exp_id}_acc_detailed_from_reps.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
