{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f7e5fd-0a5f-4cc9-bc1f-9c7d126cfe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/araymond/storage/pyenv/versions/3.10.14/envs/mini/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "There was a problem when trying to write in your cache folder (/storage/cache). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    }
   ],
   "source": [
    "from models import LightningRepClassification\n",
    "from model_info import encoders, modulators, model_output_dims\n",
    "from utils import set_seed, get_args\n",
    "import torch\n",
    "from datasets import IdSpritesEval\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_dataloader(args):\n",
    "    indices = list(range(480000))\n",
    "    data = torch.load(f\"{args.dataset}/{args.dataset}.pth\", map_location=\"cpu\")\n",
    "    if args.pretrained_reps:\n",
    "        data['reps'] = torch.load(f\"{args.dataset}/{args.dataset}_images_feats_{args.pretrained_reps}.pth\", map_location=\"cpu\")\n",
    "    ds = IdSpritesEval(args, data, indices, max_delta=14, num_samples=20, p_skip=0, test=False, return_indices=True)\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "    return dl\n",
    "def get_model(args):\n",
    "    encoder = encoders[args.encoder.arch](args) if args.encoder.arch != \"none\" else None\n",
    "    input_dims = model_output_dims[args.encoder.arch] if args.encoder.arch != \"none\" else  model_output_dims[args.pretrained_reps]\n",
    "    \n",
    "    print(\"input_dim\",input_dims,\n",
    "          \"hidden_dim\",args.modulator.hidden_dim)\n",
    "    modulator = modulators[args.train_method](input_dim=input_dims,\n",
    "                                              hidden_dim=args.modulator.hidden_dim,\n",
    "                                              latent_dim = 5 if args.dataset == \"idsprites\" else 6\n",
    "                                              )\n",
    "    # Load from checkpoint:\n",
    "    model = LightningRepClassification.load_from_checkpoint(checkpoint_path=f\"results/{args.dataset}/{exp_id}/last.ckpt\", \n",
    "                                                            args=args, \n",
    "                                                            encoder=encoder, \n",
    "                                                            modulator=modulator)\n",
    "    return model\n",
    "\n",
    "def evaluate_and_save(model, dataloader, groups_list, output_file):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    split = ['train','id','ood']\n",
    "    with torch.no_grad():\n",
    "        for n_batch, batch in enumerate(tqdm(dataloader)):\n",
    "            # Unpack index + batch\n",
    "            idxs, src_img, src_rep, imgs, gt_reps, latents = batch\n",
    "            data = model.split_step((src_img.cuda(), src_rep.cuda(), imgs.cuda(), gt_reps.cuda(), latents.cuda()))\n",
    "            # We want to store per-sample results:\n",
    "            # The logits are bs*n_classes x n_classes â€” reshape if needed.\n",
    "            bs = src_img.size(0)\n",
    "            preds = data['logits'].argmax(dim=-1).cpu().numpy()\n",
    "            targets = data['class_tgt'].cpu().numpy()\n",
    "            tasks = data['tasks'].view(-1).cpu().numpy()\n",
    "            for i, sample_idx in enumerate(idxs.cpu().numpy()):\n",
    "                result = {\n",
    "                    'idx': sample_idx,\n",
    "                    'pred': preds[i],\n",
    "                    'target': targets[i],\n",
    "                    'task': tasks[i],\n",
    "                    # Add other metrics or latent info if needed\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "    df = pd.DataFrame(results)\n",
    "    df['group'] = df['idx'].map(lambda x: split[groups_list[x].long().item()])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de684ce-285e-4b55-a703-5c1567c6ee30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab169f8a5f2e4c55ae6f376b4688cc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 768 hidden_dim 128\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1755e39463b40268ec475d233315a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 1024 hidden_dim 128\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e0056b64214745af8692b0f892a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 768 hidden_dim 128\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d595a179844c93956b7a61456053bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 768 hidden_dim 128\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c2f82402a14d348445627f52c54127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 1024 hidden_dim 128\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e87d8f2efe470c935021bb718e8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 1024 hidden_dim 128\n",
      "Recentering representations!\n",
      "Normalizing reps!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55592bae0df4d3a8445df6322ca9fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define experiment ids for checkpoint\n",
    "\n",
    "exps = ['j97giv73', # composition\n",
    "        '99fuyinh', # composition\n",
    "        '8evsuasz', # interpolation\n",
    "        'w8c3385v', # interpolation\n",
    "        'nsfoq455', # extrapolation\n",
    "        'pt7snmb1' # extrapolation\n",
    "       ] # interpolation\n",
    "# experimentos mejores\n",
    "\n",
    "for exp_id in tqdm(exps):\n",
    "    args = get_args(exp_id)\n",
    "    args.encoder['pretrain_method'] = None\n",
    "    model = get_model(args)\n",
    "    dl = get_dataloader(args)\n",
    "    train_indices = torch.load(f\"3dshapes/shapes3d_{args.sub_dataset}_train_indices.pth\")\n",
    "    train_indices, val_indices = train_test_split(train_indices, test_size = 0.1, random_state=42)\n",
    "\n",
    "    groups_list = 2*torch.ones(480000)\n",
    "    groups_list[train_indices] = 0\n",
    "    groups_list[val_indices] = 1\n",
    "    evaluate_and_save(model, dl, groups_list, f\"{exp_id}_{args.dataset}_{args.sub_dataset}_{args.pretrained_reps}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def8bd85-362e-4039-9f8f-189f4419f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset  sub_dataset     model  group    task_name  accuracy\n",
      "0   3dshapes  composition  vit-b-32     id    floor_hue  0.986631\n",
      "1   3dshapes  composition  vit-b-32     id   object_hue  0.952473\n",
      "2   3dshapes  composition  vit-b-32     id  orientation  0.871923\n",
      "3   3dshapes  composition  vit-b-32     id        scale  0.918740\n",
      "4   3dshapes  composition  vit-b-32     id        shape  0.993531\n",
      "5   3dshapes  composition  vit-b-32     id     wall_hue  0.983175\n",
      "6   3dshapes  composition  vit-b-32    ood    floor_hue  0.958850\n",
      "7   3dshapes  composition  vit-b-32    ood   object_hue  0.887724\n",
      "8   3dshapes  composition  vit-b-32    ood  orientation  0.764908\n",
      "9   3dshapes  composition  vit-b-32    ood        scale  0.899644\n",
      "10  3dshapes  composition  vit-b-32    ood        shape  0.989840\n",
      "11  3dshapes  composition  vit-b-32    ood     wall_hue  0.959663\n",
      "12  3dshapes  composition  vit-b-32  train    floor_hue  0.986366\n",
      "13  3dshapes  composition  vit-b-32  train   object_hue  0.952742\n",
      "14  3dshapes  composition  vit-b-32  train  orientation  0.887531\n",
      "15  3dshapes  composition  vit-b-32  train        scale  0.922110\n",
      "16  3dshapes  composition  vit-b-32  train        shape  0.995251\n",
      "17  3dshapes  composition  vit-b-32  train     wall_hue  0.981804\n",
      "     dataset  sub_dataset     model  group    task_name  accuracy\n",
      "0   3dshapes  composition  vit-l-32     id    floor_hue  0.982286\n",
      "1   3dshapes  composition  vit-l-32     id   object_hue  0.951314\n",
      "2   3dshapes  composition  vit-l-32     id  orientation  0.809231\n",
      "3   3dshapes  composition  vit-l-32     id        scale  0.927683\n",
      "4   3dshapes  composition  vit-l-32     id        shape  0.993531\n",
      "5   3dshapes  composition  vit-l-32     id     wall_hue  0.978786\n",
      "6   3dshapes  composition  vit-l-32    ood    floor_hue  0.963821\n",
      "7   3dshapes  composition  vit-l-32    ood   object_hue  0.882745\n",
      "8   3dshapes  composition  vit-l-32    ood  orientation  0.790429\n",
      "9   3dshapes  composition  vit-l-32    ood        scale  0.901864\n",
      "10  3dshapes  composition  vit-l-32    ood        shape  0.994920\n",
      "11  3dshapes  composition  vit-l-32    ood     wall_hue  0.948076\n",
      "12  3dshapes  composition  vit-l-32  train    floor_hue  0.981196\n",
      "13  3dshapes  composition  vit-l-32  train   object_hue  0.947375\n",
      "14  3dshapes  composition  vit-l-32  train  orientation  0.820475\n",
      "15  3dshapes  composition  vit-l-32  train        scale  0.932273\n",
      "16  3dshapes  composition  vit-l-32  train        shape  0.996189\n",
      "17  3dshapes  composition  vit-l-32  train     wall_hue  0.979263\n",
      "     dataset    sub_dataset     model  group    task_name  accuracy\n",
      "0   3dshapes  interpolation  vit-b-32     id    floor_hue  0.753010\n",
      "1   3dshapes  interpolation  vit-b-32     id   object_hue  0.610314\n",
      "2   3dshapes  interpolation  vit-b-32     id  orientation  0.689942\n",
      "3   3dshapes  interpolation  vit-b-32     id        scale  0.476974\n",
      "4   3dshapes  interpolation  vit-b-32     id        shape  0.999446\n",
      "5   3dshapes  interpolation  vit-b-32     id     wall_hue  0.708716\n",
      "6   3dshapes  interpolation  vit-b-32    ood    floor_hue  0.536196\n",
      "7   3dshapes  interpolation  vit-b-32    ood   object_hue  0.586212\n",
      "8   3dshapes  interpolation  vit-b-32    ood  orientation  0.642316\n",
      "9   3dshapes  interpolation  vit-b-32    ood        scale  0.451403\n",
      "10  3dshapes  interpolation  vit-b-32    ood        shape  0.999194\n",
      "11  3dshapes  interpolation  vit-b-32    ood     wall_hue  0.524720\n",
      "12  3dshapes  interpolation  vit-b-32  train    floor_hue  0.746398\n",
      "13  3dshapes  interpolation  vit-b-32  train   object_hue  0.617402\n",
      "14  3dshapes  interpolation  vit-b-32  train  orientation  0.677533\n",
      "15  3dshapes  interpolation  vit-b-32  train        scale  0.478523\n",
      "16  3dshapes  interpolation  vit-b-32  train        shape  0.999689\n",
      "17  3dshapes  interpolation  vit-b-32  train     wall_hue  0.709076\n",
      "     dataset    sub_dataset     model  group    task_name  accuracy\n",
      "0   3dshapes  extrapolation  vit-b-32     id    floor_hue  0.884514\n",
      "1   3dshapes  extrapolation  vit-b-32     id   object_hue  0.849767\n",
      "2   3dshapes  extrapolation  vit-b-32     id  orientation  0.659195\n",
      "3   3dshapes  extrapolation  vit-b-32     id        scale  0.697283\n",
      "4   3dshapes  extrapolation  vit-b-32     id        shape  0.986517\n",
      "5   3dshapes  extrapolation  vit-b-32     id     wall_hue  0.873458\n",
      "6   3dshapes  extrapolation  vit-b-32    ood    floor_hue  0.678899\n",
      "7   3dshapes  extrapolation  vit-b-32    ood   object_hue  0.692237\n",
      "8   3dshapes  extrapolation  vit-b-32    ood  orientation  0.583349\n",
      "9   3dshapes  extrapolation  vit-b-32    ood        scale  0.662754\n",
      "10  3dshapes  extrapolation  vit-b-32    ood        shape  0.980025\n",
      "11  3dshapes  extrapolation  vit-b-32    ood     wall_hue  0.704941\n",
      "12  3dshapes  extrapolation  vit-b-32  train    floor_hue  0.888746\n",
      "13  3dshapes  extrapolation  vit-b-32  train   object_hue  0.855270\n",
      "14  3dshapes  extrapolation  vit-b-32  train  orientation  0.650432\n",
      "15  3dshapes  extrapolation  vit-b-32  train        scale  0.688759\n",
      "16  3dshapes  extrapolation  vit-b-32  train        shape  0.988590\n",
      "17  3dshapes  extrapolation  vit-b-32  train     wall_hue  0.875718\n",
      "     dataset    sub_dataset     model  group    task_name  accuracy\n",
      "0   3dshapes  extrapolation  vit-l-32     id    floor_hue  0.878937\n",
      "1   3dshapes  extrapolation  vit-l-32     id   object_hue  0.853649\n",
      "2   3dshapes  extrapolation  vit-l-32     id  orientation  0.654163\n",
      "3   3dshapes  extrapolation  vit-l-32     id        scale  0.645968\n",
      "4   3dshapes  extrapolation  vit-l-32     id        shape  0.972472\n",
      "5   3dshapes  extrapolation  vit-l-32     id     wall_hue  0.893197\n",
      "6   3dshapes  extrapolation  vit-l-32    ood    floor_hue  0.688619\n",
      "7   3dshapes  extrapolation  vit-l-32    ood   object_hue  0.706712\n",
      "8   3dshapes  extrapolation  vit-l-32    ood  orientation  0.600225\n",
      "9   3dshapes  extrapolation  vit-l-32    ood        scale  0.630079\n",
      "10  3dshapes  extrapolation  vit-l-32    ood        shape  0.956884\n",
      "11  3dshapes  extrapolation  vit-l-32    ood     wall_hue  0.713251\n",
      "12  3dshapes  extrapolation  vit-l-32  train    floor_hue  0.883784\n",
      "13  3dshapes  extrapolation  vit-l-32  train   object_hue  0.856111\n",
      "14  3dshapes  extrapolation  vit-l-32  train  orientation  0.652567\n",
      "15  3dshapes  extrapolation  vit-l-32  train        scale  0.646716\n",
      "16  3dshapes  extrapolation  vit-l-32  train        shape  0.974935\n",
      "17  3dshapes  extrapolation  vit-l-32  train     wall_hue  0.901156\n",
      "     dataset    sub_dataset     model  group    task_name  accuracy\n",
      "0   3dshapes  interpolation  vit-l-32     id    floor_hue  0.759546\n",
      "1   3dshapes  interpolation  vit-l-32     id   object_hue  0.606049\n",
      "2   3dshapes  interpolation  vit-l-32     id  orientation  0.704904\n",
      "3   3dshapes  interpolation  vit-l-32     id        scale  0.446546\n",
      "4   3dshapes  interpolation  vit-l-32     id        shape  1.000000\n",
      "5   3dshapes  interpolation  vit-l-32     id     wall_hue  0.772171\n",
      "6   3dshapes  interpolation  vit-l-32    ood    floor_hue  0.527493\n",
      "7   3dshapes  interpolation  vit-l-32    ood   object_hue  0.585869\n",
      "8   3dshapes  interpolation  vit-l-32    ood  orientation  0.654449\n",
      "9   3dshapes  interpolation  vit-l-32    ood        scale  0.446588\n",
      "10  3dshapes  interpolation  vit-l-32    ood        shape  0.999951\n",
      "11  3dshapes  interpolation  vit-l-32    ood     wall_hue  0.568503\n",
      "12  3dshapes  interpolation  vit-l-32  train    floor_hue  0.760266\n",
      "13  3dshapes  interpolation  vit-l-32  train   object_hue  0.613802\n",
      "14  3dshapes  interpolation  vit-l-32  train  orientation  0.684643\n",
      "15  3dshapes  interpolation  vit-l-32  train        scale  0.460401\n",
      "16  3dshapes  interpolation  vit-l-32  train        shape  1.000000\n",
      "17  3dshapes  interpolation  vit-l-32  train     wall_hue  0.766648\n"
     ]
    }
   ],
   "source": [
    "exps = ['j97giv73',\n",
    "        '99fuyinh',\n",
    "        '8evsuasz',\n",
    "        'nsfoq455',\n",
    "        'pt7snmb1',\n",
    "        'w8c3385v'\n",
    "       # \"nn3235c9\",\n",
    "       # \"8p8nqlqm\",\n",
    "       # \"3j1sltd9\",\n",
    "       # \"ycdtf0ng\",\n",
    "       # \"57gubcbl\",\n",
    "       # \"uu8nayjd\"\n",
    "        ] # experimentos mejores\n",
    "#exps = [\"ga9v6jrr\", \"q77tx64m\", \"4i1m0a4x\", \"aauu4gxw\",\"6l733ceo\",\"u8ql80ja\"] # experimentos al reves\n",
    "\n",
    "final_result = pd.DataFrame()\n",
    "for exp_id in exps:\n",
    "    args = get_args(exp_id)\n",
    "    filename = f\"{exp_id}_{args.dataset}_{args.sub_dataset}_{args.pretrained_reps}.csv\"\n",
    "    exp_id, dataset, sub_dataset = filename.split(\"_\")[:3]\n",
    "    pretrained_reps = \"-\".join(filename.split(\"_\")[3:]).replace(\".csv\",\"\")\n",
    "    df= pd.read_csv(filename)\n",
    "    df['dataset'] = dataset\n",
    "    df['sub_dataset'] = sub_dataset\n",
    "    df['model'] = pretrained_reps\n",
    "    df['correct'] = df['pred'] == df['target']\n",
    "    df['task_name'] = df['task'].apply(lambda x:  [\"floor_hue\", \"wall_hue\", \"object_hue\", \"scale\", \"shape\", \"orientation\"][x])\n",
    "    result = df.groupby(['dataset','sub_dataset',\"model\",'group', 'task_name'])['correct'].mean().reset_index()\n",
    "    result.rename(columns={'correct': 'accuracy'}, inplace=True)\n",
    "    print(result)\n",
    "    final_result = pd.concat([final_result, result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac27c2d-674c-4daf-8f32-f049e1aef859",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['group'] = pd.Categorical(final_result['group'], categories=['train', 'id','ood'], ordered=True)\n",
    "final_result = final_result.sort_values(by=['dataset', 'sub_dataset','model','group'], ascending=[True, True,True,True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ff1ff8-703c-408f-82ae-8b640662b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648864/3461703406.py:6: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df_model.pivot_table(\n",
      "/tmp/ipykernel_2648864/3461703406.py:6: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df_model.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "models = ['vit-b-32', 'vit-l-32']\n",
    "df = final_result\n",
    "tables = {}\n",
    "for model in models:\n",
    "    df_model = df[df['model'] == model]\n",
    "    pivot = df_model.pivot_table(\n",
    "        index=['sub_dataset', 'group'],\n",
    "        columns='task_name',\n",
    "        values='accuracy'\n",
    "    ).reset_index()\n",
    "    tables[model] = pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f99886d-9977-4dbd-8c86-6094e8dd4048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task_name</th>\n",
       "      <th>sub_dataset</th>\n",
       "      <th>group</th>\n",
       "      <th>floor_hue</th>\n",
       "      <th>object_hue</th>\n",
       "      <th>orientation</th>\n",
       "      <th>scale</th>\n",
       "      <th>shape</th>\n",
       "      <th>wall_hue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>composition</td>\n",
       "      <td>train</td>\n",
       "      <td>0.986366</td>\n",
       "      <td>0.952742</td>\n",
       "      <td>0.887531</td>\n",
       "      <td>0.922110</td>\n",
       "      <td>0.995251</td>\n",
       "      <td>0.981804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>composition</td>\n",
       "      <td>id</td>\n",
       "      <td>0.986631</td>\n",
       "      <td>0.952473</td>\n",
       "      <td>0.871923</td>\n",
       "      <td>0.918740</td>\n",
       "      <td>0.993531</td>\n",
       "      <td>0.983175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>composition</td>\n",
       "      <td>ood</td>\n",
       "      <td>0.958850</td>\n",
       "      <td>0.887724</td>\n",
       "      <td>0.764908</td>\n",
       "      <td>0.899644</td>\n",
       "      <td>0.989840</td>\n",
       "      <td>0.959663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extrapolation</td>\n",
       "      <td>train</td>\n",
       "      <td>0.888746</td>\n",
       "      <td>0.855270</td>\n",
       "      <td>0.650432</td>\n",
       "      <td>0.688759</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>0.875718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>extrapolation</td>\n",
       "      <td>id</td>\n",
       "      <td>0.884514</td>\n",
       "      <td>0.849767</td>\n",
       "      <td>0.659195</td>\n",
       "      <td>0.697283</td>\n",
       "      <td>0.986517</td>\n",
       "      <td>0.873458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>extrapolation</td>\n",
       "      <td>ood</td>\n",
       "      <td>0.678899</td>\n",
       "      <td>0.692237</td>\n",
       "      <td>0.583349</td>\n",
       "      <td>0.662754</td>\n",
       "      <td>0.980025</td>\n",
       "      <td>0.704941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interpolation</td>\n",
       "      <td>train</td>\n",
       "      <td>0.746398</td>\n",
       "      <td>0.617402</td>\n",
       "      <td>0.677533</td>\n",
       "      <td>0.478523</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.709076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>interpolation</td>\n",
       "      <td>id</td>\n",
       "      <td>0.753010</td>\n",
       "      <td>0.610314</td>\n",
       "      <td>0.689942</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.708716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>interpolation</td>\n",
       "      <td>ood</td>\n",
       "      <td>0.536196</td>\n",
       "      <td>0.586212</td>\n",
       "      <td>0.642316</td>\n",
       "      <td>0.451403</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.524720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task_name    sub_dataset  group  floor_hue  object_hue  orientation     scale  \\\n",
       "0            composition  train   0.986366    0.952742     0.887531  0.922110   \n",
       "1            composition     id   0.986631    0.952473     0.871923  0.918740   \n",
       "2            composition    ood   0.958850    0.887724     0.764908  0.899644   \n",
       "3          extrapolation  train   0.888746    0.855270     0.650432  0.688759   \n",
       "4          extrapolation     id   0.884514    0.849767     0.659195  0.697283   \n",
       "5          extrapolation    ood   0.678899    0.692237     0.583349  0.662754   \n",
       "6          interpolation  train   0.746398    0.617402     0.677533  0.478523   \n",
       "7          interpolation     id   0.753010    0.610314     0.689942  0.476974   \n",
       "8          interpolation    ood   0.536196    0.586212     0.642316  0.451403   \n",
       "\n",
       "task_name     shape  wall_hue  \n",
       "0          0.995251  0.981804  \n",
       "1          0.993531  0.983175  \n",
       "2          0.989840  0.959663  \n",
       "3          0.988590  0.875718  \n",
       "4          0.986517  0.873458  \n",
       "5          0.980025  0.704941  \n",
       "6          0.999689  0.709076  \n",
       "7          0.999446  0.708716  \n",
       "8          0.999194  0.524720  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables['vit-b-32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ffefc8a-60b0-455e-b5bb-aa96aa100a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[ht]\n",
      "\\centering\n",
      "\\caption{Hola}\n",
      "\\label{tab:asda}\n",
      "\\begin{tabularx}{\\textwidth}{llXXXXXX}\n",
      "\\textbf{Sub dataset} & \\textbf{Group} & \\textbf{Floor hue} & \\textbf{Object hue} & \\textbf{Orientation} & \\textbf{Scale} & \\textbf{Shape} & \\textbf{Wall hue} \\\\\n",
      "Composition & TRAIN & 98.12\\% & 94.74\\% & 82.05\\% & 93.23\\% & 99.62\\% & 97.93\\% \\\\\n",
      "Composition & ID & 98.23\\% & 95.13\\% & 80.92\\% & 92.77\\% & 99.35\\% & 97.88\\% \\\\\n",
      "Composition & OOD & 96.38\\% & 88.27\\% & 79.04\\% & 90.19\\% & 99.49\\% & 94.81\\% \\\\\n",
      "Extrapolation & TRAIN & 88.38\\% & 85.61\\% & 65.26\\% & 64.67\\% & 97.49\\% & 90.12\\% \\\\\n",
      "Extrapolation & ID & 87.89\\% & 85.36\\% & 65.42\\% & 64.60\\% & 97.25\\% & 89.32\\% \\\\\n",
      "Extrapolation & OOD & 68.86\\% & 70.67\\% & 60.02\\% & 63.01\\% & 95.69\\% & 71.33\\% \\\\\n",
      "Interpolation & TRAIN & 76.03\\% & 61.38\\% & 68.46\\% & 46.04\\% & 100.00\\% & 76.66\\% \\\\\n",
      "Interpolation & ID & 75.95\\% & 60.60\\% & 70.49\\% & 44.65\\% & 100.00\\% & 77.22\\% \\\\\n",
      "Interpolation & OOD & 52.75\\% & 58.59\\% & 65.44\\% & 44.66\\% & 100.00\\% & 56.85\\% \\\\\n",
      "\\end{tabularx}\n",
      "\\end{table}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2648864/4184161582.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0    TRAIN\n",
      "1       ID\n",
      "2      OOD\n",
      "3    TRAIN\n",
      "4       ID\n",
      "5      OOD\n",
      "6    TRAIN\n",
      "7       ID\n",
      "8      OOD\n",
      "Name: group, dtype: object' has dtype incompatible with category, please explicitly cast to a compatible dtype first.\n",
      "  df_fmt.iloc[:, 1] = df_fmt.iloc[:, 1].str.upper()       # group\n"
     ]
    }
   ],
   "source": [
    "def df_to_tabularx(df, label, caption, column_width='\\\\textwidth'):\n",
    "    import io\n",
    "\n",
    "    # Copy and format DataFrame\n",
    "    df_fmt = df.copy()\n",
    "    df_fmt.iloc[:, 0] = df_fmt.iloc[:, 0].str.capitalize()  # sub_dataset\n",
    "    df_fmt.iloc[:, 1] = df_fmt.iloc[:, 1].str.upper()       # group\n",
    "\n",
    "    # Format numeric columns as percentages\n",
    "    for col in df.columns[2:]:\n",
    "        df_fmt[col] = (df[col] * 100).map(lambda x: f\"{x:.2f}\\\\%\")\n",
    "\n",
    "    # Build tabularx column format\n",
    "    num_task_columns = df_fmt.shape[1] - 2\n",
    "    column_format = 'll' + 'X' * num_task_columns\n",
    "\n",
    "    # Prepare bold column headers\n",
    "    bold_headers = [f\"\\\\textbf{{{col.replace('_', ' ').capitalize()}}}\" for col in df_fmt.columns]\n",
    "\n",
    "    # Use to_latex to get the content, skipping header\n",
    "    buf = io.StringIO()\n",
    "    df_fmt.to_latex(\n",
    "        buf,\n",
    "        index=False,\n",
    "        header=False,\n",
    "        escape=False,\n",
    "        column_format=column_format\n",
    "    )\n",
    "    lines = buf.getvalue().splitlines()\n",
    "\n",
    "    # Insert bold header manually\n",
    "    header_line = ' & '.join(bold_headers) + ' \\\\\\\\'\n",
    "    table_body = '\\n'.join(lines[3:-2])  # skip to_latex's \\toprule, etc.\n",
    "\n",
    "    # Final LaTeX table\n",
    "    latex = (\n",
    "        f\"\\\\begin{{table}}[ht]\\n\"\n",
    "        f\"\\\\centering\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\"\n",
    "        f\"\\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\begin{{tabularx}}{{{column_width}}}{{{column_format}}}\\n\"\n",
    "        f\"{header_line}\\n\"\n",
    "        f\"{table_body}\\n\"\n",
    "        f\"\\\\end{{tabularx}}\\n\"\n",
    "        f\"\\\\end{{table}}\"\n",
    "    )\n",
    "\n",
    "    return latex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label = \"tab:asda\"\n",
    "caption = \"Hola\"\n",
    "print(df_to_tabularx(tables['vit-b-32'], label, caption, column_width='\\\\textwidth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb2273-8ddb-43c4-b2f0-1073b8f7992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7043d1a8-d363-4fd3-a3e2-523846936754",
   "metadata": {},
   "outputs": [],
   "source": [
    " from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(ds, batch_size=16)\n",
    "images, reps, delta = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018b1d13-6214-4c90-bc79-3e0a67bca03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "n_classes = 10\n",
    "torch.tensor(bs*list(range(n_classes))).view(-1, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5a15fcf-4e02-4bde-810f-00114a488d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160]) torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_metrics(data):\n",
    "    criterion = F.cosine_similarity\n",
    "    metrics = dict()\n",
    "    loss = 0\n",
    "    if True:\n",
    "        same_loss = 1 - criterion(data['mid_reps'], data['rep_tgt']).mean()\n",
    "        loss += same_loss\n",
    "        metrics['same_loss'] = same_loss\n",
    "\n",
    "    if True:\n",
    "        class_loss = F.cross_entropy(data['logits'], data['class_tgt'].view(-1), reduction=\"none\").mean()\n",
    "        loss +=  class_loss\n",
    "        metrics['class_loss'] = class_loss\n",
    "\n",
    "        preds = logits.argmax(dim=-1).view(-1)\n",
    "        correct = (preds == data['class_tgt']).view(-1).float()\n",
    "        accuracy = correct.sum()/correct.numel()\n",
    "        metrics['class_acc'] = accuracy\n",
    "        dtype=correct.dtype\n",
    "        device=correct.device\n",
    "        print(correct.shape, tasks.shape)\n",
    "        mean_per_group = torch.zeros(5, dtype=dtype, device=device).scatter_reduce(0, data['tasks'], correct, reduce=\"mean\")\n",
    "        for i, task in enumerate(['shape','scale','orientation','x','y']):\n",
    "            metrics[f'class_{task}'] = mean_per_group[i]\n",
    "            \n",
    "    metrics['loss'] = loss\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        metrics[k] = v.item()\n",
    "    return metrics \n",
    "    \n",
    "imgs, gt_reps, latents = images, reps, delta \n",
    "zero_latents = torch.zeros_like(latents)\n",
    "deltas = latents.sum(dim=-1)\n",
    "bs, n_classes, *_ = imgs.shape\n",
    "\n",
    "mid_reps = gt_reps if True else self.encoder(imgs)     # Image encoding\n",
    "reps = torch.randn((bs,n_classes,128))                       # predicted reps given latents\n",
    "tgt_reps = torch.randn((bs,n_classes,128))               # reps we are trying to achieve\n",
    "\n",
    "logits = torch.matmul(reps, tgt_reps.transpose(1,2)).view(-1, n_classes) # bs x 10 x 10 --> 10bs x 10\n",
    "#reps, valid_indices = expand_reps(reps, ranges) # valid indices means which lines to keep on the loss\n",
    "#tgt_reps, _ = expand_reps(tgt_reps, ranges)\n",
    "data['mid_reps'] = mid_reps\n",
    "data['rep_tgt'] = gt_reps\n",
    "data['logits'] = logits\n",
    "targets = torch.tensor(bs*list(range(n_classes))).view(-1, n_classes)\n",
    "tasks = latents.abs().argmax(dim=-1)\n",
    "data['class_tgt'] = targets.view(-1)\n",
    "data['tasks'] = tasks.view(-1)\n",
    "metrics = get_metrics(data)\n",
    "# altered encoded rep must be equal to rep of original image\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51095491-0a28-4561-9e6b-6bee16240213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'same_loss': 0.0,\n",
       " 'class_loss': 18.06698226928711,\n",
       " 'class_acc': 0.09375,\n",
       " 'class_shape': 0.06060606241226196,\n",
       " 'class_scale': 0.09090909361839294,\n",
       " 'class_orientation': 0.09090909361839294,\n",
       " 'class_x': 0.0882352963089943,\n",
       " 'class_y': 0.125,\n",
       " 'loss': 18.06698226928711}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb1990e-12d1-47da-965c-0c6fabb9d943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  0,  0,  0],\n",
       "        [ 0,  1,  0,  0,  0],\n",
       "        [ 0,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0,  1,  0],\n",
       "        [ 0,  0,  0,  0,  1],\n",
       "        [ 0,  0,  0,  0, -1],\n",
       "        [ 2,  0,  0,  0,  0],\n",
       "        [ 0,  2,  0,  0,  0],\n",
       "        [ 0,  0,  2,  0,  0],\n",
       "        [ 0,  0,  0,  2,  0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a583389-0aef-4061-9879-d6f76f97c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2ed1636e0f4d7fabc88f5b6cf0324f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_=10 max_=10\r"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "min_ = float('inf')\n",
    "max_ = float('-inf')\n",
    "for idx, item in tqdm(enumerate(ds)):\n",
    "    min_ = min(item[-1].size(0), min_)\n",
    "    max_ = max(item[-1].size(0), max_)\n",
    "    if min_ < 10: \n",
    "        print('min_', idx)\n",
    "        break\n",
    "    if max_ > 10: \n",
    "        print('max_', idx)\n",
    "        break\n",
    "    print(f'min_={min_} max_={max_}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf12d33-83ac-4696-b259-538529242d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01dc6bcf-134c-40ac-9e91-edd6045b2b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8aecdb9-27fe-4b3a-8580-f67b8d9d5bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=100, latent_dim=5, hidden_dim=128, n_blocks=3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj = nn.Linear(latent_dim,input_dim)\n",
    "        modules = [nn.Linear(2*self.input_dim, self.hidden_dim)]\n",
    "        for i in range(n_blocks-1):\n",
    "            modules.append(nn.ReLU())\n",
    "            modules.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "\n",
    "        l = self.proj(l)\n",
    "        x = torch.cat((x,l), dim=1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = MLP(input_dim=100, hidden_dim=32, latent_dim=5, n_blocks=4)\n",
    "data = torch.randn(64,100)\n",
    "l = torch.randn(64,5)\n",
    "model(data, l).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "477b0db8-61a8-40c0-8c3b-70e7c30014c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "encoders = {\n",
    "    None: None,\n",
    "    \"vit\": 384,\n",
    "    \"vit_b_16\": 1,    \n",
    "    \"vit_b_32\": 22,\n",
    "    \"vit_l_16\": 16,\n",
    "    \"vit_l_32\": 32\n",
    "}\n",
    "\n",
    "print(encoders[\"vit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f0340f-9ebb-49ec-8690-580372192f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "root = 'idsprites'\n",
    "\n",
    "data  = torch.load(f\"{root}/idsprites.pth\")\n",
    "data['reps'] = torch.load(f\"{root}/idsprites_images_feats_vit_b_16.pth\")\n",
    "train_indices = torch.tensor([i for i in range(len(data['images']))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87db024-bed2-4926-b544-ed8ef52adfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b967c9fa-e44f-44f7-b7ce-b74ada870fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa0d68-942d-4bc3-ab14-99e975f21efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_latents_to_values(new_latents_idxs):\n",
    "    new_latents_idxs = new_latents_idxs.long()\n",
    "    new_latents_idxs[:,1:] = torch.clamp(new_latents_idxs[:,1:], min=0, max=13)\n",
    "    latent_indices = f.one_hot(new_latents_idxs[:,1:],num_classes=14)\n",
    "    new_latents = torch.cat((new_latents_idxs[:,0].unsqueeze(1),(latent_indices*values).sum(dim=-1)),dim=1)\n",
    "    return new_latents\n",
    "    \n",
    "def map_detail(x):\n",
    "    l = ['shape+','scale+','orientation+','x+','y+',\n",
    "        'shape-','scale-','orientation-','x-','y-']\n",
    "    return  l[x]\n",
    "    \n",
    "def index_to_latent_id(idx):\n",
    "    shape = (idx // (14**4)) % 54\n",
    "    scale =  (idx // (14**3)) % 14\n",
    "    orientation =  (idx // (14**2)) % 14\n",
    "    x =  (idx // 14) % 14\n",
    "    y =  idx % 14\n",
    "    return (shape,scale,orientation,x,y)\n",
    "\n",
    "# latent_id = tensor with original starting latent_ids for all latent attributes (size = (1,5))\n",
    "# delta = how many steps to move in all latent attributes\n",
    "\n",
    "def get_delta_latents(latent_id, delta):\n",
    "    \n",
    "    pred_delta_latents = delta*torch.eye(5)\n",
    "    pred_delta_latents = torch.cat((pred_delta_latents,-pred_delta_latents), dim=0)\n",
    "    pred_delta_latents = pred_delta_latents.repeat(n,1,1) # CPU\n",
    "    \n",
    "    new_latents_idxs = (latent_id.repeat(1,10,1) + pred_delta_latents).to(torch.int8) # CPU\n",
    "    out_of_min_range = new_latents_idxs >= 0\n",
    "    out_of_max_range = new_latents_idxs < 14\n",
    "    out_of_max_range[:,:,0] = new_latents_idxs[:,:,0] < 54\n",
    "    out_of_range = out_of_max_range*out_of_min_range\n",
    "    viable_latents = torch.all(out_of_range,dim=2).view(-1)\n",
    "\n",
    "def latent_id_to_split(latent, ood):\n",
    "    latent = {k: v for k, v in zip(['shape','scale','orientation','x','y'], latent)}\n",
    "    for k, v in ood.items():\n",
    "        if latent[k] in v:\n",
    "            return \"ood\"\n",
    "    else:\n",
    "        return \"iid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ba41b-a0ec-411c-8c75-d14b848caef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7be1ab-8dbd-440b-8b33-f2da5842344b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e26ff-6c7e-471d-bf0c-4b4740dd2340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b223e70-c15d-44bd-bc92-1d6e3228c10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d57cd-2a3b-437f-8452-4f55c3cb4c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f1846-b12f-434e-bfec-7ae10a5a3000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
