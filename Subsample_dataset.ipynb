{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80040ce9-16ee-4a8b-9577-5b23e9e5aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsample dataset for testing\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "split = \"abstraction\"\n",
    "\n",
    "partition = \"test\"\n",
    "data_dir = f\"/mnt/nas2/GrimaRepo/araymond/3dshapes/shapes3d_{split}_{partition}_\"\n",
    "\n",
    "\n",
    "images = np.load(dadta_dir + \"images.npz\")\n",
    "labels = np.load(data_dir + \"labels.npz\")\n",
    "\n",
    "num_samples = len(images['arr_0'])//10\n",
    "indices = np.random.choice(len(images['arr_0']), num_samples, replace=False)\n",
    "\n",
    "new_images = images['arr_0'][indices]\n",
    "new_labels = labels['arr_0'][indices]\n",
    "\n",
    "images_file = data_dir + \"images_subsample_for_evaluation_in_training.npz\"\n",
    "labels_file = data_dir + \"labels_subsample_for_evaluation_in_training.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a3e153-ffc8-4223-8466-d5798f44e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(images_file, new_images)\n",
    "np.savez(labels_file, new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b3ef50-b065-47d8-84b2-b794130f782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(new_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa0bb530-0304-4d6d-ae7a-fd4e75b1f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = np.random.choice(len(images['arr_0']), num_samples, replace=False)\n",
    "\n",
    "new_images = images['arr_0'][indices]\n",
    "new_labels = labels['arr_0'][indices]\n",
    "\n",
    "images_file = data_dir + \"images_subsample_for_evaluation_in_training.npz\"\n",
    "labels_file = data_dir + \"labels_subsample_for_evaluation_in_training.npz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801ef48-ad7a-40e7-add3-f0f8b4fd2548",
   "metadata": {},
   "source": [
    "## Get Features for split for ViT architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac198a-1368-4b37-a351-cbb4a5a17956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/araymond/storage/pyenv/versions/3.10.14/envs/mini/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "There was a problem when trying to write in your cache folder (/storage/cache). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8008179f12b843c9bce73e3a76e5d1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83b4cf2693846b2aa6865d04cf1504d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0799842336224ae791d77185169f4a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_b_16, Dataset: 14, Split: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4838505ed24f39a49833e645a9aeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_b_32, Dataset: 14, Split: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd641bdbeec4c17a4b8717da57c0a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_l_16, Dataset: 14, Split: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9544badd69b943128d9a9073787bdba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_l_32, Dataset: 14, Split: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93311b31ed9f4ad0a3e2ff64bfea2f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad16d3b3fd334ac7bdeb84a4ac7c26cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0d4c9922f44d4586520bfe82aa3889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_b_16, Dataset: 14, Split: shape\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a853a484b3664feea92f2f79f0f70f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_b_32, Dataset: 14, Split: shape\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d42e40badd64a9482f3cd352ef1c7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently getting reps for arch: vit_l_16, Dataset: 14, Split: shape\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6650affcdba4a4ebc39c503dda3ad84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "# Create preprocessed model\n",
    "import os\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = \"~/storage/cache\"\n",
    "arch = \"vit_b_16\" # vit_b_16, vit_b_32, vit_l_16, vit_l_32\n",
    "dataset = \"idsprites\"\n",
    "splits = ['train','shape',\"scale\",'orientation','x','y']\n",
    "constructors = {\n",
    "    \"vit_b_16\": models.vit_b_16,\n",
    "    \"vit_b_32\": models.vit_b_32,\n",
    "    \"vit_l_16\": models.vit_l_16,\n",
    "    \"vit_l_32\": models.vit_l_32   \n",
    "}\n",
    "\n",
    "model_weights ={\n",
    "    \"vit_b_16\": models.ViT_B_16_Weights.IMAGENET1K_V1,\n",
    "    \"vit_b_32\": models.ViT_B_32_Weights.IMAGENET1K_V1,\n",
    "    \"vit_l_16\": models.ViT_L_16_Weights.IMAGENET1K_V1,\n",
    "    \"vit_l_32\": models.ViT_L_32_Weights.IMAGENET1K_V1   \n",
    "}\n",
    "\n",
    "def get_images_path(dataset, feat_args):\n",
    "    split = feat_args['split']\n",
    "    arch = feat_args['arch']\n",
    "    n_shapes = feat_args['n_shapes'] if 'n_shapes' in feat_args else None\n",
    "    polation = feat_args['polation'] if 'polation' in feat_args else None\n",
    "    \n",
    "    if dataset == \"shapes3d\":\n",
    "        return f\"/mnt/nas2/GrimaRepo/araymond/shapes3d/shapes3d_abstraction_{split}_images.npz\"\n",
    "    elif dataset == \"idsprites\":\n",
    "        return f\"idsprites/idsprites_{polation}_{n_shapes}_images_{split}.npz\"\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset} not supported!\")\n",
    "\n",
    "def get_output_path(dataset, feat_args):\n",
    "\n",
    "    split = feat_args['split']\n",
    "    arch = feat_args['arch']\n",
    "    n_shapes = feat_args['n_shapes'] if 'n_shapes' in feat_args else None\n",
    "    polation = feat_args['polation'] if 'polation' in feat_args else None\n",
    "    \n",
    "    if dataset == \"shapes3d\":\n",
    "        return f\"/mnt/nas2/GrimaRepo/araymond/shapes3d/shapes3d_abstraction_{split}_images_feats_{arch}.npz\"\n",
    "    elif dataset == \"idsprites\":\n",
    "           return f\"/mnt/nas2/GrimaRepo/araymond/idsprites/idsprites_{polation}_{n_shapes}_images_feats_{split}_{arch}.npz\"\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset} not supported!\")\n",
    "\n",
    "feat_args = {'split': None, 'arch': None, 'polation': 'inter', 'shapes': None}\n",
    "\n",
    "for split in tqdm(splits):\n",
    "    feat_args['split'] = split\n",
    "    \n",
    "    for n_shapes in tqdm([14]):\n",
    "        feat_args['n_shapes'] = n_shapes\n",
    "        # data_dir = f\"/mnt/nas2/GrimaRepo/araymond//shapes3d_abstraction_{split}_images.npz\"\n",
    "        data_dir = get_images_path(dataset, feat_args)\n",
    "        images = np.load(data_dir)['arr_0']\n",
    "        # Data\n",
    "        tensor_images = torch.from_numpy(images)\n",
    "        ds = TensorDataset(tensor_images)\n",
    "        dl = DataLoader(ds, batch_size=512, shuffle=False)\n",
    "        \n",
    "        for arch in tqdm([\"vit_b_16\", \"vit_b_32\", \"vit_l_16\", \"vit_l_32\"]):\n",
    "            print(f\"Currently getting reps for arch: {arch}, Dataset: {n_shapes}, Split: {split}\")\n",
    "            feat_args['arch'] = arch\n",
    "            # Load a pretrained Vision Transformer model\n",
    "            vit_model = constructors[arch](weights=model_weights[arch]).cuda()\n",
    "            # Get the preprocessing transforms for the model\n",
    "            preprocess = model_weights[arch].transforms()\n",
    "            # Run the image through the model\n",
    "            vit_model.heads = torch.nn.Identity() # We want to extract features\n",
    "            vit_model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "            result = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x in tqdm(dl):\n",
    "                    x = x[0].cuda()\n",
    "                    input_tensor = preprocess(x)  # Add batch dimension\n",
    "                    output = vit_model(input_tensor)\n",
    "                    result.append(output.cpu().detach())\n",
    "            \n",
    "            v = torch.vstack(result)    \n",
    "            results_path = get_output_path(dataset, feat_args)\n",
    "            np.savez(results_path, v.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3475399c-7c79-4bd4-bd10-97543b97e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading pretrained model vit_b_16...\n",
      "Obtaining representations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facb2c8c61064dcf9de361efd99a617f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.int8)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tqdm(dl):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 80\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m()\n\u001b[1;32m     81\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m preprocess(x)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     output \u001b[38;5;241m=\u001b[39m vit_model(input_tensor)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "# Create preprocessed model\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Example of argparse usage\")\n",
    "\n",
    "parser.add_argument('--arch', type=str, help='Architecture to extract features from')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args = edict()\n",
    "args.arch = \"vit_b_16\"\n",
    "os.environ[\"TORCH_HOME\"] = \"~/storage/cache\"\n",
    "#arch = \"vit_b_16\" # vit_b_16, vit_b_32, vit_l_16, vit_l_32\n",
    "dataset = \"idsprites\"\n",
    "splits = [ 'train','shape',\"scale\",\n",
    "\t\t'orientation','x','y']\n",
    "constructors = {\n",
    "    \"vit_b_16\": models.vit_b_16,\n",
    "    \"vit_b_32\": models.vit_b_32,\n",
    "    \"vit_l_16\": models.vit_l_16,\n",
    "    \"vit_l_32\": models.vit_l_32   \n",
    "}\n",
    "\n",
    "model_weights ={\n",
    "    \"vit_b_16\": models.ViT_B_16_Weights.IMAGENET1K_V1,\n",
    "    \"vit_b_32\": models.ViT_B_32_Weights.IMAGENET1K_V1,\n",
    "    \"vit_l_16\": models.ViT_L_16_Weights.IMAGENET1K_V1,\n",
    "    \"vit_l_32\": models.ViT_L_32_Weights.IMAGENET1K_V1   \n",
    "}\n",
    "\n",
    "\n",
    "def get_output_path(dataset, feat_args):\n",
    "\n",
    "    split = feat_args['split']\n",
    "    arch = feat_args['arch']\n",
    "    n_shapes = feat_args['n_shapes'] if 'n_shapes' in feat_args else None\n",
    "    polation = feat_args['polation'] if 'polation' in feat_args else None\n",
    "    \n",
    "    if dataset == \"shapes3d\":\n",
    "        return f\"/mnt/nas2/GrimaRepo/araymond/shapes3d/shapes3d_abstraction_{split}_images_feats_{arch}.npz\"\n",
    "    elif dataset == \"idsprites\":\n",
    "           return f\"idsprites/idsprites_images_feats_{arch}.pth\"\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset} not supported!\")\n",
    "\n",
    "\n",
    "\n",
    "# data_dir = f\"/mnt/nas2/GrimaRepo/araymond//shapes3d_abstraction_{split}_images.npz\"\n",
    "data_dir = f\"idsprites/idsprites.pth\"\n",
    "\n",
    "print(\"Loading dataset...\", flush=True)\n",
    "tensor_images = torch.load(data_dir)['images']\n",
    "# Data\n",
    "#tensor_images = torch.from_numpy(images)\n",
    "ds = TensorDataset(tensor_images)\n",
    "dl = DataLoader(ds, batch_size=512, shuffle=False)\n",
    "arch = args.arch\n",
    "\n",
    "# Load a pretrained Vision Transformer model\n",
    "print(f\"Loading pretrained model {args.arch}...\", flush=True)\n",
    "vit_model = constructors[arch](weights=model_weights[arch]).cuda()\n",
    "# Get the preprocessing transforms for the model\n",
    "preprocess = model_weights[arch].transforms()\n",
    "# Run the image through the model\n",
    "vit_model.heads = torch.nn.Identity() # We want to extract features\n",
    "vit_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "result = []\n",
    "\n",
    "print(\"Obtaining representations...\", flush=True)\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(dl):\n",
    "        print(x)\n",
    "        x = x.cuda()\n",
    "        input_tensor = preprocess(x)  # Add batch dimension\n",
    "        output = vit_model(input_tensor)\n",
    "        result.append(output.cpu().detach())\n",
    "\n",
    "v = torch.vstack(result)    \n",
    "results_path = get_output_path(dataset, feat_args)\n",
    "torch.save(v, results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b8b907-4b2f-4bde-9ff6-ac3b1920a405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a0c978-9b03-48f5-aa97-ea4f9d357ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "data = torch.load(\"idsprites/idsprites.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225bbf7d-df6d-4bda-88df-039fbb9e4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = dict()\n",
    "ds_size = len(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fddde80b-e32c-4afc-9df3-80e8dd26a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(list(range(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e5bacd4-c018-46a5-bf8c-2fab2c6c1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['images'] = data['images'][indices]\n",
    "new_data['latents'] = data['latents'][indices]\n",
    "new_data['latent_ids'] = data['latents'][indices]\n",
    "new_data['meta'] = data['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9227ba5-168c-40f6-9774-0d379c1d0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_data, \"idsprites/idsprites_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec16580-5407-4ffc-9308-c94591b8103a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
